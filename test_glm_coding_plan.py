#!/usr/bin/env python
"""
–¢–µ—Å—Ç GLM Coding Plan endpoint.
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.
"""

import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

def test_glm_coding_plan():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç GLM Coding Plan endpoint."""
    
    api_key = os.getenv("GLM_API_KEY")
    
    if not api_key:
        print("‚ùå GLM_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
        return
    
    print("=" * 80)
    print("–¢–ï–°–¢ GLM CODING PLAN ENDPOINT")
    print("=" * 80)
    
    # –¢–µ—Å—Ç 1: –û–±—ã—á–Ω—ã–π endpoint
    print("\n1. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ–≥–æ GLM endpoint...")
    print(f"   Endpoint: https://open.bigmodel.cn/api/paas/v4/")
    print(f"   –ú–æ–¥–µ–ª—å: glm-4-flash")
    
    try:
        client = OpenAI(
            api_key=api_key,
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            timeout=30
        )
        
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞–µ—Ç?"}
            ],
            max_tokens=50
        )
        
        result = response.choices[0].message.content
        print(f"   ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω: {result}")
        print(f"   ‚úÖ –û–±—ã—á–Ω—ã–π endpoint —Ä–∞–±–æ—Ç–∞–µ—Ç!")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        print(f"   ‚ùå –û–±—ã—á–Ω—ã–π endpoint –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    
    # –¢–µ—Å—Ç 2: Coding Plan endpoint
    print("\n2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GLM Coding Plan endpoint...")
    print(f"   Endpoint: https://api.z.ai/api/coding/paas/v4/")
    print(f"   –ú–æ–¥–µ–ª—å: glm-4.7")
    
    try:
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.z.ai/api/coding/paas/v4/",
            timeout=30
        )
        
        print("   –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
        
        response = client.chat.completions.create(
            model="glm-4.7",
            messages=[
                {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞–µ—Ç?"}
            ],
            max_tokens=50
        )
        
        result = response.choices[0].message.content
        print(f"   ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω: {result}")
        print(f"   ‚úÖ Coding Plan endpoint —Ä–∞–±–æ—Ç–∞–µ—Ç!")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        print(f"   ‚ùå Coding Plan endpoint –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print(f"\n   üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"      1. –£ –≤–∞—Å –Ω–µ—Ç –ø–æ–¥–ø–∏—Å–∫–∏ Coding Plan")
        print(f"      2. API –∫–ª—é—á –Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω –∫ Coding Plan")
        print(f"      3. –ú–æ–¥–µ–ª—å glm-4.7 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        print(f"      4. Endpoint –∏–∑–º–µ–Ω–∏–ª—Å—è")
    
    print("\n" + "=" * 80)
    print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("=" * 80)
    print("\nüí° –ï—Å–ª–∏ Coding Plan endpoint –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:")
    print("   1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—ã—á–Ω—ã–π GLM endpoint (–æ—Ç–∫–ª—é—á–∏—Ç–µ —á–µ–∫–±–æ–∫—Å)")
    print("   2. –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Groq (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∏ –±—ã—Å—Ç—Ä—ã–π)")
    print("\n‚úÖ –ï—Å–ª–∏ Coding Plan endpoint —Ä–∞–±–æ—Ç–∞–µ—Ç:")
    print("   1. –í–∫–ª—é—á–∏—Ç–µ —á–µ–∫–±–æ–∫—Å '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Coding Plan' –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
    print("   2. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å glm-4.7, glm-4.6, glm-4.5 –∏–ª–∏ glm-4.5-air")

if __name__ == "__main__":
    test_glm_coding_plan()
